{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import api_keys_v2\n",
    "from google.cloud.api_keys_v2 import Key\n",
    "\n",
    "def create_api_key(project_id: str, suffix: str) -> Key:\n",
    "    \"\"\"\n",
    "    Creates and restrict an API key. Add the suffix for uniqueness.\n",
    "    (Developer):\n",
    "    1. Before running this sample,\n",
    "      set up ADC as described in https://cloud.google.com/docs/authentication/external/set-up-adc\n",
    "    2. Make sure you have the necessary permission to create API keys.\n",
    "    Args:\n",
    "        project_id: Google Cloud project id.\n",
    "    Returns:\n",
    "        response: Returns the created API Key.\n",
    "    \"\"\"\n",
    "    # Create the API Keys client.\n",
    "    client = api_keys_v2.ApiKeysClient()\n",
    "\n",
    "    key = api_keys_v2.Key()\n",
    "    key.display_name = f\"API key - {suffix}\"\n",
    "\n",
    "    # Initialize request and set arguments.\n",
    "    request = api_keys_v2.CreateKeyRequest()\n",
    "    request.parent = f\"projects/{project_id}/locations/global\"\n",
    "    request.key = key\n",
    "\n",
    "    # Make the request and wait for the operation to complete.\n",
    "    response = client.create_key(request=request).result()\n",
    "\n",
    "    print(f\"Successfully created an API key: {response.name}\")\n",
    "    # For authenticating with the API key, use the value in \"response.key_string\".\n",
    "    # To restrict the usage of this API key, use the value in \"response.name\".\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel, ChatModel, InputOutputTextPair, CodeChatModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM for general purposes\n",
    "def google_llm(inquiry):\n",
    "    vertexai.init(project='project_name', location='us-central1')\n",
    "    # TODO developer - override these parameters as needed:\n",
    "    parameters = {\n",
    "        \"temperature\": 0.9,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": 1024,  # Token limit determines the maximum amount of text output.\n",
    "        \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "        \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    response = model.predict(\n",
    "        inquiry,\n",
    "        **parameters,\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM for call center\n",
    "def google_chat(message, temperature: float = 0.2):\n",
    "    chat_model = ChatModel.from_pretrained(\"chat-bison@001\")\n",
    "\n",
    "    # TODO developer - override these parameters as needed:\n",
    "    parameters = {\n",
    "        \"temperature\": temperature,  # Temperature controls the degree of randomness in token selection.\n",
    "        \"max_output_tokens\": 256,  # Token limit determines the maximum amount of text output.\n",
    "        \"top_p\": 0.95,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "        \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "    }\n",
    "\n",
    "    chat = chat_model.start_chat(\n",
    "        context=\"My name is Bot. You are a call center supervisor, knowledgable of the best practices.\",\n",
    "        examples=[\n",
    "            InputOutputTextPair(\n",
    "                input_text=\"What are the main mistakes when answering a call?\",\n",
    "                output_text=\"Not start by introducing yourself and voicing your interest in solving the clients needs\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    response = chat.send_message(\n",
    "        message, **parameters\n",
    "    )\n",
    "\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM for code\n",
    "def google_code(inquiry):\n",
    "    vertexai.init(project=\"project_name\", location=\"us-central1\")\n",
    "    chat_model = CodeChatModel.from_pretrained(\"codechat-bison\")\n",
    "    parameters = {\n",
    "        \"candidate_count\": 1,\n",
    "        \"max_output_tokens\": 1024,\n",
    "        \"temperature\": 0.9\n",
    "    }\n",
    "    chat = chat_model.start_chat()\n",
    "    response = chat.send_message(\n",
    "        inquiry, \n",
    "        **parameters\n",
    "    )\n",
    "    \n",
    "    return response.text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
